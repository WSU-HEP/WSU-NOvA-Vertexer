{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# `x_model_prediction.py` \n",
    "## Generate a prediction from the x model\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1c548d2f092a4ee6"
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "a844c5ac892f54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "from tensorflow.python.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# TODO: add the argparse block\n",
    "\n",
    "\n",
    "# Load the SavedModel\n",
    "file_model = '/Users/michaeldolce/Development/files/h5-files/models/X_FHC_Model.h5'\n",
    "# Locations on the WSU HPC in Abdul's directory...\n",
    "# '/home/m962g264/wsu_Nova_Vertexer/output/h5_models/model_082023/X_FHC_Model.h5'\n",
    "# '/home/m962g264/wsu_Nova_Vertexer/output/h5_models/model_082023/X_RHC_Model.h5'\n",
    "\n",
    "model = load_model(file_model)\n",
    "\n",
    "# Load the designated test file. This is file 27\n",
    "# NOTE: there is only one test file for the FD validation, and it should be within the \"test\" directory\n",
    "# ANOTHER NOTE: the test file has not been pre-processed, so it has all information within it! So can investigate deeper the interactions from each vertex.  \n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-13T21:44:55.777308Z"
    }
   },
   "id": "a8d2b6eb"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c36045fbc04a972",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-13T21:45:14.249364Z",
     "start_time": "2023-09-13T21:45:14.205816Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test files validation path: \t/Users/michaeldolce/Development/files/h5-files/train-data/\n",
      "Files in the test directory: \t1\n"
     ]
    }
   ],
   "source": [
    "# the files in this directory are ready to be used for making predictions.\n",
    "test_path = '/home/k948d562/output/trained-models/make-predictions' \n",
    "for filename in test_path:\n",
    "    if args.detector and args.horn and args.flux in filename:\n",
    "        print('found validation file: ', filename)\n",
    "        test_path += '/' + filename\n",
    "    continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of validation files: 1\n",
      "Number of validation events: 78\n"
     ]
    }
   ],
   "source": [
    "# Checking the size of the test file(s)\n",
    "      \n",
    "test_files = [n for n in os.listdir(test_path) if n.endswith(\".h5\")] # un-used\n",
    "events=0\n",
    "for h5_filename in os.listdir(test_path):\n",
    "    events += len((h5py.File(test_path + h5_filename, 'r')['run'][:]))\n",
    "    \n",
    "print('Number of validation files:', len(os.listdir(test_path)))\n",
    "print('Number of validation events:', events)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-13T21:45:16.090332Z",
     "start_time": "2023-09-13T21:45:16.003495Z"
    }
   },
   "id": "a5a3e896"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6b9f652f31f1bd4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-13T21:45:17.173649Z",
     "start_time": "2023-09-13T21:45:17.152596Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['E', 'Mode', 'PDG', 'TrueRecoVtxX', 'TrueRecoVtxY', 'TrueRecoVtxZ', 'cvnmap', 'cycle', 'event', 'firstplane', 'isCC', 'lastcellx', 'lastcelly', 'run', 'slice', 'subrun']\n"
     ]
    }
   ],
   "source": [
    "f=h5py.File(test_path + os.listdir(test_path)[0],'r') # reading the first file in the directory\n",
    "print(list(f.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test files read successfully and packed into arrays\n"
     ]
    }
   ],
   "source": [
    "# for reading all test files\n",
    "idx_event = 0\n",
    "test_cvnmap, test_mode, test_iscc, E, pdg, true_reco_vtx_x = [], [], [], [], [], []\n",
    "# truerecovtxy, truerecovtxz -- not used here, X only.\n",
    "\n",
    "for h5_filename in os.listdir(test_path):\n",
    "    if os.path.isdir(h5_filename): #skipping directories in the files\n",
    "        continue\n",
    "    \n",
    "    print('Processing... {} of {}'.format(idx_event, len(os.listdir(test_path))), end=\"\\r\", flush=True)\n",
    "    \n",
    "    with h5py.File(test_path + h5_filename, 'r') as idx_file:\n",
    "        test_cvnmap.append(idx_file['cvnmap'][:])\n",
    "        test_mode = np.append(test_mode, idx_file['Mode'][:], axis=0)\n",
    "        test_iscc = np.append(test_iscc, idx_file['isCC'][:], axis=0)\n",
    "        true_reco_vtx_x.append(idx_file['TrueRecoVtxX'][:])\n",
    "        E = np.append(E, idx_file['E'][:], axis=0)\n",
    "        pdg = np.append(pdg, idx_file['PDG'][:], axis=0)\n",
    "        \n",
    "    \n",
    "    idx_event += 1\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "true_reco_vtx_x = np.array(true_reco_vtx_x)\n",
    "\n",
    "\n",
    "print('Test files read successfully and packed into arrays', flush=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-13T21:45:18.380631Z",
     "start_time": "2023-09-13T21:45:18.294664Z"
    }
   },
   "id": "780219df18e2bdc"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Energy (GeV) : [3.14603209 1.40381277] \n",
      " PDG code : [14. 14.] \n"
     ]
    }
   ],
   "source": [
    "# Some simple manipulation to see what's within the files...\n",
    "\n",
    "# print out this number of the last entries...\n",
    "print_count = input( \"Enter number of entries to print (from the back): \")\n",
    "\n",
    "# print(test_mode)\n",
    "# print('------')\n",
    "# print(test_iscc)\n",
    "\n",
    "# elect to print out info from these two vars...\n",
    "dict_vars = {'Energy (GeV)' : E, 'PDG code' : pdg}\n",
    "\n",
    "for dict_pair in dict_vars.items():\n",
    "    print(' {} : {} '.format(dict_pair[0], dict_pair[1][-int(print_count):]))\n",
    "\n",
    "# this is the identical as above...\n",
    "# print ('last entry: ', dict_vars['Energy (GeV)'][-print_count:])\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-13T21:45:21.185346Z",
     "start_time": "2023-09-13T21:45:19.198432Z"
    }
   },
   "id": "301efe80"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Normalizing pixel maps via sklearn preprocessing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "23e42f93b79f7571"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing preprocessing complete\n",
      "True vertex x shape: (78,) , 1\n"
     ]
    }
   ],
   "source": [
    "# True vertex extraction for analysis\n",
    "true_vtx_x, reco_vtx_x = ([] for i in range(2))\n",
    "idx_file = 0\n",
    "\n",
    "while idx_file < len(os.listdir(test_path)):\n",
    "    print('Processing...', end=\"\\r\", flush=True)\n",
    "    event = 0\n",
    "    \n",
    "    # don't quite understand this here...\n",
    "    while event < true_reco_vtx_x[idx_file].shape[0]:\n",
    "        true_vtx_x = np.append(true_vtx_x, true_reco_vtx_x[idx_file][event][0])\n",
    "        reco_vtx_x = np.append(reco_vtx_x, true_reco_vtx_x[idx_file][event][1])\n",
    "        event += 1\n",
    "    \n",
    "    idx_file += 1\n",
    "\n",
    "print('Testing preprocessing complete\\n', end=\"\\r\", flush=True)\n",
    "\n",
    "# convert to np arrays\n",
    "true_vtx_x=np.array(true_vtx_x)\n",
    "reco_vtx_x=np.array(reco_vtx_x)\n",
    "\n",
    "print('True vertex x shape: {} , {}'.format(true_vtx_x.shape, true_vtx_x.ndim))\n",
    "#print(true_vtx_x)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-13T21:45:24.586846Z",
     "start_time": "2023-09-13T21:45:24.571043Z"
    }
   },
   "id": "b5480d0f"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files: 1\n",
      "Test files normalized successfully\n",
      "(1, 78, 16000)\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "idx_event = idx_file = 0\n",
    "test_cvnmap_norm = []\n",
    "\n",
    "print('Number of files: {}'.format(len(os.listdir(test_path))))\n",
    "\n",
    "# Normalize the pixel map from test_cvnmap array from each file...\n",
    "while idx_file < (len(os.listdir(test_path))):\n",
    "    test_cvnmap_norm.append(preprocessing.normalize(test_cvnmap[idx_file], axis=1))\n",
    "    idx_file+=1\n",
    "# convert to np array\n",
    "test_cvnmap_norm=np.array(test_cvnmap_norm)\n",
    "print('Test files normalized successfully', flush=True)\n",
    "# print(test_cvnmap_norm[-1:])\n",
    "print(test_cvnmap_norm.shape)\n",
    "print(test_cvnmap_norm.ndim)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-13T21:45:27.168514Z",
     "start_time": "2023-09-13T21:45:26.634115Z"
    }
   },
   "id": "6043e1eb4f2d4427"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e9ec967",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-13T21:45:28.560488Z",
     "start_time": "2023-09-13T21:45:28.504928Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing tests cvnmap file 1 of 1\n",
      "(78, 16000)\n",
      "78\n",
      "ncvnmap processing complete\n",
      "(78, 100, 80) 3\n"
     ]
    }
   ],
   "source": [
    "# split normalized cvnmap into reshaped events with multi-views\n",
    "# todo: do not really understand this...\n",
    "c, d, test_cvnmap_norm_resh, test_cvnmap_norm_resh_xz, test_cvnmap_norm_resh_yz = ([] for i in range(5))\n",
    "idx_file, event = 0, 0\n",
    "\n",
    "while idx_file < len(os.listdir(test_path)):\n",
    "    print('Processing tests cvnmap file {} of {}\\n'.format(idx_file + 1, len(os.listdir(test_path))), end=\"\\r\", flush=True)\n",
    "    c = test_cvnmap_norm[idx_file]\n",
    "    if idx_file == 0: \n",
    "        print(c.shape)\n",
    "        print(c.shape[0])\n",
    "    event = 0\n",
    "    \n",
    "    while event < c.shape[0]:\n",
    "        d = c[event].reshape(2, 100, 80)\n",
    "        test_cvnmap_norm_resh.append(d)\n",
    "        test_cvnmap_norm_resh_xz.append(d[0])\n",
    "        test_cvnmap_norm_resh_yz.append(d[1])\n",
    "        event += 1\n",
    "    \n",
    "    idx_file += 1\n",
    "\n",
    "print('ncvnmap processing complete')\n",
    "\n",
    "test_cvnmap_norm_resh_xz=np.array(test_cvnmap_norm_resh_xz) # xz views only\n",
    "test_cvnmap_norm_resh_yz=np.array(test_cvnmap_norm_resh_yz) # yz views only\n",
    "\n",
    "print(test_cvnmap_norm_resh_xz.shape, test_cvnmap_norm_resh_xz.ndim)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff5ef08",
   "metadata": {},
   "source": [
    "**np array conversion for test preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "62c9cbc1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-13T21:45:30.781982Z",
     "start_time": "2023-09-13T21:45:30.760809Z"
    }
   },
   "outputs": [],
   "source": [
    "# add one more dimension to let the CNN know we are dealing with one color dimension\n",
    "test_cvnmap_norm_resh_xz=test_cvnmap_norm_resh_xz.reshape(test_cvnmap_norm_resh_xz.shape[0],100,80,1)\n",
    "test_cvnmap_norm_resh_yz=test_cvnmap_norm_resh_yz.reshape(test_cvnmap_norm_resh_yz.shape[0],100,80,1)\n",
    "# batch_size (you mean events?), width, height , color_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57e9fffa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-13T21:45:33.418439Z",
     "start_time": "2023-09-13T21:45:32.208876Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.python.distribute.input_lib' has no attribute 'DistributedDatasetInterface'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[17], line 8\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# import tensorflow as tf\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# model = tf.distribute.DistributedDataset()\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m      6\u001B[0m \n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m# todo: what is going on here? is information missing from the test files? (e.g. the Y and Z views?)\u001B[39;00m\n\u001B[0;32m----> 8\u001B[0m full_predictions\u001B[38;5;241m=\u001B[39m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43mtest_cvnmap_norm_resh_xz\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_cvnmap_norm_resh_yz\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py:1711\u001B[0m, in \u001B[0;36mModel.predict\u001B[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[1;32m   1705\u001B[0m   \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m:\n\u001B[1;32m   1706\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mUsing Model.predict with \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m   1707\u001B[0m                   \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMultiWorkerDistributionStrategy or TPUStrategy and \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m   1708\u001B[0m                   \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mAutoShardPolicy.FILE might lead to out-of-order result\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m   1709\u001B[0m                   \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m. Consider setting it to AutoShardPolicy.DATA.\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m-> 1711\u001B[0m data_handler \u001B[38;5;241m=\u001B[39m \u001B[43mdata_adapter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_data_handler\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1712\u001B[0m \u001B[43m    \u001B[49m\u001B[43mx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1713\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1714\u001B[0m \u001B[43m    \u001B[49m\u001B[43msteps_per_epoch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msteps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1715\u001B[0m \u001B[43m    \u001B[49m\u001B[43minitial_epoch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1716\u001B[0m \u001B[43m    \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1717\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_queue_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_queue_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1718\u001B[0m \u001B[43m    \u001B[49m\u001B[43mworkers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mworkers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1719\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_multiprocessing\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_multiprocessing\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1720\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1721\u001B[0m \u001B[43m    \u001B[49m\u001B[43msteps_per_execution\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_steps_per_execution\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1723\u001B[0m \u001B[38;5;66;03m# Container that configures and calls `tf.keras.Callback`s.\u001B[39;00m\n\u001B[1;32m   1724\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(callbacks, callbacks_module\u001B[38;5;241m.\u001B[39mCallbackList):\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/python/keras/engine/data_adapter.py:1398\u001B[0m, in \u001B[0;36mget_data_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m   1396\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m], \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_cluster_coordinator\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m   1397\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m _ClusterCoordinatorDataHandler(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m-> 1398\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mDataHandler\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/python/keras/engine/data_adapter.py:1152\u001B[0m, in \u001B[0;36mDataHandler.__init__\u001B[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001B[0m\n\u001B[1;32m   1149\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_steps_per_execution \u001B[38;5;241m=\u001B[39m steps_per_execution\n\u001B[1;32m   1150\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_steps_per_execution_value \u001B[38;5;241m=\u001B[39m steps_per_execution\u001B[38;5;241m.\u001B[39mnumpy()\u001B[38;5;241m.\u001B[39mitem()\n\u001B[0;32m-> 1152\u001B[0m adapter_cls \u001B[38;5;241m=\u001B[39m \u001B[43mselect_data_adapter\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1153\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_adapter \u001B[38;5;241m=\u001B[39m adapter_cls(\n\u001B[1;32m   1154\u001B[0m     x,\n\u001B[1;32m   1155\u001B[0m     y,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1164\u001B[0m     distribution_strategy\u001B[38;5;241m=\u001B[39mdistribute_lib\u001B[38;5;241m.\u001B[39mget_strategy(),\n\u001B[1;32m   1165\u001B[0m     model\u001B[38;5;241m=\u001B[39mmodel)\n\u001B[1;32m   1167\u001B[0m strategy \u001B[38;5;241m=\u001B[39m distribute_lib\u001B[38;5;241m.\u001B[39mget_strategy()\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/python/keras/engine/data_adapter.py:988\u001B[0m, in \u001B[0;36mselect_data_adapter\u001B[0;34m(x, y)\u001B[0m\n\u001B[1;32m    986\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mselect_data_adapter\u001B[39m(x, y):\n\u001B[1;32m    987\u001B[0m \u001B[38;5;250m  \u001B[39m\u001B[38;5;124;03m\"\"\"Selects a data adapter than can handle a given x and y.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 988\u001B[0m   adapter_cls \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mcls\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m \u001B[38;5;28mcls\u001B[39m \u001B[38;5;129;01min\u001B[39;00m ALL_ADAPTER_CLS \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39mcan_handle(x, y)]\n\u001B[1;32m    989\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m adapter_cls:\n\u001B[1;32m    990\u001B[0m     \u001B[38;5;66;03m# TODO(scottzhu): This should be a less implementation-specific error.\u001B[39;00m\n\u001B[1;32m    991\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    992\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFailed to find data adapter that can handle \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    993\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minput: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m, \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m    994\u001B[0m             _type_name(x), _type_name(y)))\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/python/keras/engine/data_adapter.py:988\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    986\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mselect_data_adapter\u001B[39m(x, y):\n\u001B[1;32m    987\u001B[0m \u001B[38;5;250m  \u001B[39m\u001B[38;5;124;03m\"\"\"Selects a data adapter than can handle a given x and y.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 988\u001B[0m   adapter_cls \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mcls\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m \u001B[38;5;28mcls\u001B[39m \u001B[38;5;129;01min\u001B[39;00m ALL_ADAPTER_CLS \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcan_handle\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m]\n\u001B[1;32m    989\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m adapter_cls:\n\u001B[1;32m    990\u001B[0m     \u001B[38;5;66;03m# TODO(scottzhu): This should be a less implementation-specific error.\u001B[39;00m\n\u001B[1;32m    991\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    992\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFailed to find data adapter that can handle \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    993\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minput: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m, \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m    994\u001B[0m             _type_name(x), _type_name(y)))\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/python/keras/engine/data_adapter.py:707\u001B[0m, in \u001B[0;36mDatasetAdapter.can_handle\u001B[0;34m(x, y)\u001B[0m\n\u001B[1;32m    704\u001B[0m \u001B[38;5;129m@staticmethod\u001B[39m\n\u001B[1;32m    705\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcan_handle\u001B[39m(x, y\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m    706\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m (\u001B[38;5;28misinstance\u001B[39m(x, (data_types\u001B[38;5;241m.\u001B[39mDatasetV1, data_types\u001B[38;5;241m.\u001B[39mDatasetV2)) \u001B[38;5;129;01mor\u001B[39;00m\n\u001B[0;32m--> 707\u001B[0m           \u001B[43m_is_distributed_dataset\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/python/keras/engine/data_adapter.py:1699\u001B[0m, in \u001B[0;36m_is_distributed_dataset\u001B[0;34m(ds)\u001B[0m\n\u001B[1;32m   1698\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_is_distributed_dataset\u001B[39m(ds):\n\u001B[0;32m-> 1699\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(ds, \u001B[43minput_lib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mDistributedDatasetInterface\u001B[49m)\n",
      "\u001B[0;31mAttributeError\u001B[0m: module 'tensorflow.python.distribute.input_lib' has no attribute 'DistributedDatasetInterface'"
     ]
    }
   ],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "# model = tf.distribute.DistributedDataset()\n",
    "\n",
    "# todo: what is going on here? is information missing from the test files? (e.g. the Y and Z views?)\n",
    "full_predictions=model.predict([test_cvnmap_norm_resh_xz, test_cvnmap_norm_resh_yz])\n",
    "\n",
    "\n",
    "#Saving full Prediction CSV file\n",
    "df_true_x_vtx=pd.DataFrame(true_vtx_x, columns=['True X'])\n",
    "df_sub_true_x_vtx = df_true_x_vtx.iloc[:10, :10]  # Select the first 10 rows and first 10 columns\n",
    "df_reco_x_vtx=pd.DataFrame(reco_vtx_x, columns=['Reco X'])\n",
    "df_sub_reco_x_vtx= df_reco_x_vtx.iloc[:10, :10]\n",
    "full_predictions=pd.Series(full_predictions.reshape(len(true_vtx_x), ))\n",
    "df_true_x_vtx=pd.concat([df_true_x_vtx, full_predictions], axis=1)\n",
    "subset_concat_df = df_true_x_vtx.iloc[:10, :10]\n",
    "df_true_x_vtx=pd.concat([df_true_x_vtx, df_reco_x_vtx], axis=1)\n",
    "subset_concate_df = df_true_x_vtx.iloc[:10, :10]\n",
    "df_true_x_vtx.columns=['True X', 'Model Predictions', 'Reco X']\n",
    "print('The first 10 rows and first 10 columns of truevtxx are:', df_sub_true_x_vtx)\n",
    "print('The first 10 rows and first 10 columns of recovtxx are:', df_sub_reco_x_vtx)\n",
    "print('The first 10 rows and first 10 columns of concat truevtxx and prediction are:', subset_concat_df)\n",
    "print('The first 10 rows and first 10 columns of concat truevtxx, prediction and recovtxx are:', subset_concate_df)\n",
    "np.savetxt(\"/Users/michaeldolce/Development/files/csv-files/trial_x_modelPred_FHC_abdul.csv\", df_true_x_vtx, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head of the pandas df....\n",
      "   Energy (GeV)  PDG code   CC      True X      Reco X\n",
      "0      2.272811      14.0  0.0   -5.490415   -3.578348\n",
      "1      1.645375      14.0  1.0   95.721954   94.725662\n",
      "2      1.756794      14.0  1.0  148.567261  148.680298\n",
      "3      4.959294      14.0  0.0 -116.638466 -109.047287\n",
      "4     20.487104     -12.0  1.0  -28.045332  -26.292982\n"
     ]
    }
   ],
   "source": [
    "# Saving the true and reco vtx x values to a csv file\n",
    "\n",
    "# True info\n",
    "df_true_x_vtx=pd.DataFrame(true_vtx_x, columns=['True X'])\n",
    "df_sub_true_x_vtx = df_true_x_vtx.iloc[:10, :10]  # Select the first 10 rows and first 10 columns\n",
    "\n",
    "# Reco info\n",
    "df_reco_x_vtx=pd.DataFrame(reco_vtx_x, columns=['Reco X'])\n",
    "df_sub_reco_x_vtx= df_reco_x_vtx.iloc[:10, :10]\n",
    "\n",
    "df_E = pd.DataFrame(E, columns=['Energy (GeV)'])\n",
    "df_pdg = pd.DataFrame(pdg, columns=['PDG code'])\n",
    "df_CC = pd.DataFrame(test_iscc, columns=['CC'])\n",
    "\n",
    "df_vars=pd.concat([df_E, df_pdg, df_CC, df_true_x_vtx, df_reco_x_vtx], axis=1)\n",
    "# df_vars.columns=['True X', 'Reco X'] # in case we want to rename the columns...\n",
    "\n",
    "# TODO: add the energies and PDGs and CC of the information to the csv file...\n",
    "# TODO: change isCC to a string instead...\n",
    "# TODO: and then make plots of them (decide later if they should be a new plotting macro or the existing one from Abdul)\n",
    "\n",
    "print('head of the pandas df....')\n",
    "print(df_vars.head())\n",
    "np.savetxt(\"/Users/michaeldolce/Development/files/csv-files/x_model_fhc_df_vars.csv\", df_vars, delimiter=\",\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-13T22:05:22.043965Z",
     "start_time": "2023-09-13T22:05:22.033348Z"
    }
   },
   "id": "5be8500feffeb500"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ef5774291c5a23bd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
